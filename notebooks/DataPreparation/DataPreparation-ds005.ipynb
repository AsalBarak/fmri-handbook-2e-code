{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates the processed data for dataset ds005, which is used in some of the chapter-specific notebooks from Poldrack, Mumford, and Nichols' _Handbook of fMRI Data Analysis (2nd Edition)_.  This also provides an example of using the nipype workflow mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poldrack/anaconda/envs/py3k/lib/python3.5/importlib/_bootstrap_external.py:903: FutureWarning: Module nipy.labs.utils.routines deprecated, will be removed\n",
      "  _imp.create_dynamic, spec)\n",
      "/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/nipype/interfaces/nipy/model.py:18: FutureWarning: Module nipy.labs.glm deprecated, will be removed. Please use nipy.modalities.fmri.glm instead.\n",
      "  import nipy.labs.glm.glm as GLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using base dir: /Users/poldrack/data/fmri-handbook-2e-data\n"
     ]
    }
   ],
   "source": [
    "import os, errno, sys,shutil\n",
    "\n",
    "from fmrihandbook.utils.config import Config\n",
    "\n",
    "config=Config()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from nipype.interfaces import fsl, nipy, ants\n",
    "import nibabel\n",
    "from wand.image import Image as WImage\n",
    "import numpy\n",
    "import nilearn.plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import fmrihandbook.utils\n",
    "from fmrihandbook.utils.compute_fd_dvars import compute_fd,compute_dvars\n",
    "import pickle\n",
    "from fmrihandbook.utils.get_data import get_data\n",
    "\n",
    "import nipype.interfaces.io as nio           # Data i/o\n",
    "import nipype.pipeline.engine as pe          # pypeline engine\n",
    "import nipype.algorithms.modelgen as model   # model specification\n",
    "from nipype.interfaces.base import Bunch\n",
    "import glob\n",
    "import nipype.interfaces.utility as niu\n",
    "from nipype.interfaces.c3 import C3dAffineTool\n",
    "from nipype.interfaces.utility import Merge, IdentityInterface\n",
    "\n",
    "\n",
    "rerun_analyses=False  # set to true to force rerun of everything\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ds005': {'datadir': '/Users/poldrack/data/fmri-handbook-2e-data/ds005'}, 'ds031': {'fieldmap-mag': '/Users/poldrack/data/fmri-handbook-2e-data/ds031/sub-01/ses-014/fmap/sub-01_ses-014_magnitude1.nii.gz', 'T1': '/Users/poldrack/data/fmri-handbook-2e-data/ds031/sub-01/ses-018/anat/sub-01_ses-018_run-001_T1w.nii.gz', 'sbref': '/Users/poldrack/data/fmri-handbook-2e-data/ds031/sub-01/ses-014/func/sub-01_ses-014_task-nback_run-001_sbref.nii.gz', 'func': '/Users/poldrack/data/fmri-handbook-2e-data/ds031/sub-01/ses-014/func/sub-01_ses-014_task-nback_run-001_bold.nii.gz', 'fieldmap-phasediff': '/Users/poldrack/data/fmri-handbook-2e-data/ds031/sub-01/ses-014/fmap/sub-01_ses-014_phasediff.nii.gz', 'T2': '/Users/poldrack/data/fmri-handbook-2e-data/ds031/sub-01/ses-018/anat/sub-01_ses-018_run-001_T2w.nii.gz', 'datadir': '/Users/poldrack/data/fmri-handbook-2e-data/ds031'}}\n"
     ]
    }
   ],
   "source": [
    "config.data=get_data('ds005')\n",
    "print(config.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-01']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subcodes=[os.path.basename(i) for i in glob.glob(os.path.join(config.data['ds005']['datadir'],'sub-*'))]\n",
    "subcodes.sort()\n",
    "subcodes=subcodes[:1]\n",
    "print(subcodes)\n",
    "\n",
    "# Map field names to individual subject runs.\n",
    "info = dict(func=[['subject_id','subject_id','runcode']],\n",
    "            anat=[['subject_id', 'subject_id']])\n",
    "\n",
    "infosource = pe.Node(interface=niu.IdentityInterface(fields=['subject_id','runcode']), name=\"infosource\")\n",
    "\n",
    "\"\"\"Here we set up iteration over all the subjects. The following line\n",
    "is a particular example of the flexibility of the system.  The\n",
    "``datasource`` attribute ``iterables`` tells the pipeline engine that\n",
    "it should repeat the analysis on each of the items in the\n",
    "``subject_list``. In the current example, the entire first level\n",
    "preprocessing and estimation will be repeated for each subject\n",
    "contained in subject_list.\n",
    "\"\"\"\n",
    "\n",
    "infosource.iterables = [('subject_id', subcodes),('runcode',['1','2','3'])]\n",
    "\n",
    "\n",
    "datasource_anat = pe.Node(interface=nio.DataGrabber(infields=['subject_id'],\n",
    "                    outfields=['anat']),\n",
    "                    name = 'datasource')\n",
    "\n",
    "datasource_anat.inputs.base_directory = config.data['ds005']['datadir']\n",
    "\n",
    "datasource_anat.inputs.template = '%s/anat/%s_T1w.nii.gz'\n",
    "datasource_anat.inputs.template_args =dict(anat=[['subject_id', 'subject_id']])\n",
    "#datasource_anat.inputs.subject_id = subcodes\n",
    "datasource_anat.inputs.sort_filelist = True\n",
    "#datasource_anat.iterables = ('subject_id', subcodes)\n",
    "\n",
    "                     \n",
    "                                                   \n",
    "preprocessing = pe.Workflow(name=\"preprocessing\")\n",
    "workdir=os.path.join(config.data['ds005']['datadir'],'nipype_workdir')\n",
    "if not os.path.exists(workdir):\n",
    "    os.mkdir(workdir)\n",
    "preprocessing.base_dir = workdir\n",
    "\n",
    "preprocessing.connect(infosource,'subject_id',datasource_anat,'subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated. You should import from ipykernel or jupyter_client instead.\n",
      "  \"You should import from ipykernel or jupyter_client instead.\", ShimWarning)\n",
      "/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/IPython/parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n",
      "INFO:workflow:['check', 'execution', 'logging']\n",
      "INFO:workflow:Running serially.\n",
      "INFO:workflow:Executing node datasource.a1 in dir: /Users/poldrack/data/fmri-handbook-2e-data/ds005/nipype_workdir/preprocessing/_runcode_2_subject_id_sub-01/datasource\n",
      "INFO:workflow:Executing node datasource.a0 in dir: /Users/poldrack/data/fmri-handbook-2e-data/ds005/nipype_workdir/preprocessing/_runcode_1_subject_id_sub-01/datasource\n",
      "INFO:workflow:Executing node datasource.a2 in dir: /Users/poldrack/data/fmri-handbook-2e-data/ds005/nipype_workdir/preprocessing/_runcode_3_subject_id_sub-01/datasource\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x1042b2748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias field correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-01']\n"
     ]
    }
   ],
   "source": [
    "print(subcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bfc = pe.Node(interface=ants.N4BiasFieldCorrection(), name=\"bfc\")\n",
    "bfc.inputs.dimension = 3\n",
    "bfc.inputs.save_bias = True\n",
    "\n",
    "preprocessing.connect(datasource_anat, 'anat', bfc, 'input_image')\n",
    "\n",
    "datasink = pe.Node(nio.DataSink(), name='datasink')\n",
    "regex_subs = [('_combiner.*/sar', '/smooth/'),\n",
    "              ('_combiner.*/ar', '/unsmooth/'),\n",
    "              ('_aparc_ts.*/sar', '/smooth/'),\n",
    "              ('_aparc_ts.*/ar', '/unsmooth/'),\n",
    "              ('_getsubcortts.*/sar', '/smooth/'),\n",
    "              ('_getsubcortts.*/ar', '/unsmooth/'),\n",
    "              ('series/sar', 'series/smooth/'),\n",
    "              ('series/ar', 'series/unsmooth/'),\n",
    "              ('_inverse_transform./', ''),\n",
    "              ]\n",
    "# Save the relevant data into an output directory\n",
    "datasink.inputs.base_directory = os.path.join(config.data['ds005']['datadir'],'derivatives')\n",
    "if not os.path.exists(datasink.inputs.base_directory):\n",
    "    os.mkdir(datasink.inputs.base_directory)\n",
    "\n",
    "#datasink.inputs.substitutions = substitutions\n",
    "#datasink.inputs.regexp_substitutions = regex_subs #(r'(/_.*(\\d+/))', r'/run\\2')\n",
    "#datasink.inputs.container=os.path.join()\n",
    "\n",
    "preprocessing.connect(bfc, 'bias_image', datasink, 'bfc.bias')\n",
    "preprocessing.connect(bfc, 'output_image', datasink, 'bfc.output')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain extraction using BET###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet_struct=pe.Node(interface=fsl.BET(), name=\"bet_struct\")\n",
    "bet_struct.inputs.reduce_bias=True\n",
    "bet_struct.inputs.frac=0.4\n",
    "\n",
    "preprocessing.connect(bfc,'output_image',bet_struct,'in_file')\n",
    "preprocessing.connect(bet_struct, 'out_file', datasink, 'bet.output')\n",
    "preprocessing.connect(bet_struct, 'mask_file', datasink, 'bet.mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation using FAST\n",
    "\n",
    "Do this to obtain the white matter mask, which we need for BBR registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast=pe.Node(interface=fsl.FAST(), name=\"fast\")\n",
    "\n",
    "preprocessing.connect(bet_struct,'out_file',fast,'in_files')\n",
    "preprocessing.connect(fast, 'partial_volume_files', datasink, 'fast.pvefiles')\n",
    "preprocessing.connect(fast, 'tissue_class_map', datasink, 'fast.seg')\n",
    "\n",
    "binarize = pe.Node(fsl.ImageMaths(op_string='-nan -thr 0.5 -bin'),\n",
    "                   name='binarize')\n",
    "pickindex = lambda x, i: x[i]\n",
    "preprocessing.connect(fast, ('partial_volume_files', pickindex, 2),\n",
    "                 binarize, 'in_file')\n",
    "preprocessing.connect(binarize, 'out_file', datasink, 'fast.wmseg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial normalization using ANTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "antsreg=pe.Node(interface=ants.Registration(), name=\"antsreg\")\n",
    "antsreg.inputs.fixed_image = os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz')\n",
    "antsreg.inputs.transforms = ['Translation', 'Rigid', 'Affine', 'SyN']\n",
    "antsreg.inputs.transform_parameters = [(0.1,), (0.1,), (0.1,), (0.2, 3.0, 0.0)]\n",
    "antsreg.inputs.number_of_iterations = ([[10, 10, 10]]*3 +\n",
    "                [[1, 5, 3]])\n",
    "antsreg.inputs.dimension = 3\n",
    "antsreg.inputs.write_composite_transform = True\n",
    "antsreg.inputs.metric = ['Mattes'] * 3 + [['Mattes', 'CC']]\n",
    "antsreg.inputs.metric_weight = [1] * 3 + [[0.5, 0.5]]\n",
    "antsreg.inputs.radius_or_number_of_bins = [32] * 3 + [[32, 4]]\n",
    "antsreg.inputs.sampling_strategy = ['Regular'] * 3 + [[None, None]]\n",
    "antsreg.inputs.sampling_percentage = [0.3] * 3 + [[None, None]]\n",
    "antsreg.inputs.convergence_threshold = [1.e-8] * 3 + [-0.01]\n",
    "antsreg.inputs.convergence_window_size = [20] * 3 + [5]\n",
    "antsreg.inputs.smoothing_sigmas = [[4, 2, 1]] * 3 + [[1, 0.5, 0]]\n",
    "antsreg.inputs.sigma_units = ['vox'] * 4\n",
    "antsreg.inputs.shrink_factors = [[6, 4, 2]] + [[3, 2, 1]]*2 + [[4, 2, 1]]\n",
    "antsreg.inputs.use_estimate_learning_rate_once = [True] * 4\n",
    "antsreg.inputs.use_histogram_matching = [False] * 3 + [True]\n",
    "antsreg.inputs.initial_moving_transform_com = True\n",
    "antsreg.inputs.output_warped_image = True\n",
    "\n",
    "preprocessing.connect(bet_struct,'out_file',antsreg,'moving_image')\n",
    "preprocessing.connect(antsreg, 'warped_image', datasink, 'ants.warped_image')\n",
    "preprocessing.connect(antsreg, 'composite_transform', datasink, 'ants.composite_transform')\n",
    "preprocessing.connect(antsreg, 'inverse_composite_transform', datasink, 'ants.inverse_composite_transform')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional preprocessing\n",
    "\n",
    "### Motion correction using MCFLIRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasource_func = pe.Node(interface=nio.DataGrabber(infields=['subject_id','runcode'],\n",
    "                                               outfields=['func']),\n",
    "                     name = 'datasource_func')\n",
    "datasource_func.inputs.base_directory = config.data['ds005']['datadir']\n",
    "\n",
    "datasource_func.inputs.template = '%s/func/%s_task-mixedgamblestask_run-0%s_bold.nii.gz'\n",
    "datasource_func.inputs.template_args = dict(func=[['subject_id','subject_id','runcode']])\n",
    "#datasource_anat.inputs.subject_id = subcodes\n",
    "datasource_func.inputs.sort_filelist = True\n",
    "#datasource_func.iterables = ('runcode',['1','2','3'])\n",
    "\n",
    "preprocessing.connect(infosource,'subject_id',datasource_func,'subject_id')\n",
    "preprocessing.connect(infosource,'runcode',datasource_func,'runcode')\n",
    "\n",
    "\n",
    "\n",
    "mcflirt=pe.Node(interface=fsl.MCFLIRT(), name=\"mcflirt\")\n",
    "mcflirt.save_plots=True\n",
    "mcflirt.mean_vol=True\n",
    "\n",
    "preprocessing.connect(datasource_func, 'func', mcflirt, 'in_file')\n",
    "\n",
    "preprocessing.connect(mcflirt, 'out_file', datasink, 'mcflirt.out_file')\n",
    "preprocessing.connect(mcflirt, 'par_file', datasink, 'mcflirt.par')\n",
    "preprocessing.connect(mcflirt, 'mean_img', datasink, 'mcflirt.mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make links for the mean functional image and the motion parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain extraction\n",
    "\n",
    "Use FSL's BET to obtain the brain mask for the functional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet_func=pe.Node(interface=fsl.BET(), name=\"bet_func\")\n",
    "\n",
    "bet_func.inputs.functional=True\n",
    "bet_func.inputs.mask=True\n",
    "\n",
    "preprocessing.connect(mcflirt, 'out_file', bet_func, 'in_file')\n",
    "\n",
    "preprocessing.connect(bet_func, 'out_file', datasink, 'betfunc.out_file')\n",
    "preprocessing.connect(bet_func, 'mask_file', datasink, 'betfunc.mask_file')\n",
    "\n",
    "meanbetfunc=pe.Node(interface=fsl.MeanImage(), name=\"meanbetfunc\")\n",
    "preprocessing.connect(bet_func,'out_file',meanbetfunc,'in_file')\n",
    "preprocessing.connect(meanbetfunc, 'out_file', datasink, 'betfunc.mean_file')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBR registration of functional to structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean2anat = pe.Node(fsl.FLIRT(), name='mean2anat')\n",
    "mean2anat.inputs.dof = 6\n",
    "preprocessing.connect(meanbetfunc, 'out_file', mean2anat, 'in_file')\n",
    "preprocessing.connect(bet_struct, 'out_file', mean2anat, 'reference')\n",
    "\n",
    "mean2anatbbr = pe.Node(fsl.FLIRT(), name='mean2anatbbr')\n",
    "mean2anatbbr.inputs.dof = 6\n",
    "mean2anatbbr.inputs.cost = 'bbr'\n",
    "mean2anatbbr.inputs.schedule = os.path.join(os.getenv('FSLDIR'),\n",
    "                                            'etc/flirtsch/bbr.sch')\n",
    "\n",
    "preprocessing.connect(meanbetfunc, 'out_file', mean2anatbbr, 'in_file')\n",
    "preprocessing.connect(binarize, 'out_file', mean2anatbbr, 'wm_seg')\n",
    "preprocessing.connect(bet_struct, 'out_file', mean2anatbbr, 'reference')\n",
    "preprocessing.connect(mean2anat, 'out_matrix_file',\n",
    "                 mean2anatbbr, 'in_matrix_file')\n",
    "\n",
    "preprocessing.connect(mean2anatbbr, 'out_matrix_file', datasink, 'bbr.out_matrix')\n",
    "preprocessing.connect(mean2anatbbr, 'out_file', datasink, 'bbr.out_file')\n",
    "\n",
    "# convert BBR matrix to ITK for ANTS\n",
    "\n",
    "convert2itk = pe.Node(C3dAffineTool(),\n",
    "                      name='convert2itk')\n",
    "convert2itk.inputs.fsl2ras = True\n",
    "convert2itk.inputs.itk_transform = True\n",
    "preprocessing.connect(mean2anatbbr, 'out_matrix_file', convert2itk, 'transform_file')\n",
    "preprocessing.connect(meanbetfunc, 'out_file', convert2itk, 'source_file')\n",
    "preprocessing.connect(bet_struct, 'out_file', convert2itk, 'reference_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated. You should import from ipykernel or jupyter_client instead.\n",
      "  \"You should import from ipykernel or jupyter_client instead.\", ShimWarning)\n",
      "/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/IPython/parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n",
      "INFO:workflow:['check', 'execution', 'logging']\n",
      "INFO:workflow:Running serially.\n",
      "INFO:workflow:Executing node datasource.a2 in dir: /Users/poldrack/data/fmri-handbook-2e-data/ds005/nipype_workdir/preprocessing/_runcode_3_subject_id_sub-01/datasource\n",
      "ERROR:workflow:['Node datasource.a2 failed to run on host Russells-MacBook-Pro.local.']\n",
      "INFO:workflow:Saving crash info to /Users/poldrack/code/fmri-handbook-2e-code/notebooks/DataPreparation/crash-20160104-113400-poldrack-datasource.a2.pklz\n",
      "INFO:workflow:Traceback (most recent call last):\n",
      "  File \"/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/nipype/pipeline/plugins/linear.py\", line 39, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/nipype/pipeline/engine/nodes.py\", line 392, in run\n",
      "    self._run_interface()\n",
      "  File \"/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/nipype/pipeline/engine/nodes.py\", line 502, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/nipype/pipeline/engine/nodes.py\", line 640, in _run_command\n",
      "    dirs2keep=dirs2keep)\n",
      "  File \"/Users/poldrack/anaconda/envs/py3k/lib/python3.5/site-packages/nipype/pipeline/engine/utils.py\", line 1116, in clean_working_directory\n",
      "    output_files.extend(walk_outputs(outputdict[output]))\n",
      "KeyError: 'anat'\n",
      "\n",
      "INFO:workflow:Executing node datasource_func.a2 in dir: /Users/poldrack/data/fmri-handbook-2e-data/ds005/nipype_workdir/preprocessing/_runcode_3_subject_id_sub-01/datasource_func\n",
      "INFO:workflow:Executing node mcflirt.a2 in dir: /Users/poldrack/data/fmri-handbook-2e-data/ds005/nipype_workdir/preprocessing/_runcode_3_subject_id_sub-01/mcflirt\n",
      "INFO:workflow:Running: mcflirt -in /Users/poldrack/data/fmri-handbook-2e-data/ds005/sub-01/func/sub-01_task-mixedgamblestask_run-03_bold.nii.gz -out /Users/poldrack/Dropbox/data/fmri-handbook-2e-data/ds005/nipype_workdir/preprocessing/_runcode_3_subject_id_sub-01/mcflirt/sub-01_task-mixedgamblestask_run-03_bold_mcf.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the affine and ants transforms into a list\n",
    "\n",
    "pickfirst = lambda x: x[0]\n",
    "\n",
    "merge = pe.Node(Merge(2), iterfield=['in2'], name='mergexfm')\n",
    "preprocessing.connect(convert2itk, 'itk_transform', merge, 'in2')\n",
    "preprocessing.connect(antsreg, 'composite_transform', merge, 'in1')\n",
    "\n",
    "warpmean = pe.Node(ants.ApplyTransforms(), name='warpmean')\n",
    "warpmean.inputs.input_image_type = 0\n",
    "warpmean.inputs.interpolation = 'Linear'\n",
    "warpmean.inputs.invert_transform_flags = [False, False]\n",
    "warpmean.inputs.terminal_output = 'file'\n",
    "warpmean.inputs.args = '--float'\n",
    "warpmean.inputs.reference_image = os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz')\n",
    "\n",
    "warpall = pe.MapNode(ants.ApplyTransforms(),\n",
    "                     iterfield=['input_image'],\n",
    "                     name='warpall')\n",
    "warpall.inputs.input_image_type = 0\n",
    "warpall.inputs.interpolation = 'Linear'\n",
    "warpall.inputs.invert_transform_flags = [False, False]\n",
    "warpall.inputs.terminal_output = 'file'\n",
    "warpall.inputs.args = '--float'\n",
    "warpall.inputs.num_threads = 2\n",
    "warpall.plugin_args = {'sbatch_args': '--mem=6G -c 2'}\n",
    "warpall.inputs.reference_image = os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz')\n",
    "\n",
    "\n",
    "preprocessing.connect(meanbetfunc, 'out_file', warpmean, 'input_image')\n",
    "preprocessing.connect(merge, 'out', warpmean, 'transforms')\n",
    "preprocessing.connect(bet_func, 'out_file', warpall, 'input_image')\n",
    "preprocessing.connect(merge, 'out', warpall, 'transforms')\n",
    "\n",
    "graph=preprocessing.run()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for subcode in subcodes:\n",
    "    antsregfile=os.path.join(config.data['ds005']['datadir'],\n",
    "                             'derivatives/ants/warped_image/_subject_id_%s/transform_Warped.nii.gz'%subcode)\n",
    "    if os.path.exists(antsregfile):\n",
    "        mask_display=nilearn.plotting.plot_epi(anstregfile,cmap='gray')\n",
    "        mask_display.add_contours(os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz'),\n",
    "                                  levels=[.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and plot the global signal within the mask across timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "niftimasker=NiftiMasker(mask_img=config.data['meanfunc_brain_mask'])\n",
    "bolddata_masked=niftimasker.fit_transform(config.data['func_mcf'])\n",
    "globalmean=numpy.mean(bolddata_masked,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA for fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load motion data and compute FD/DVARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "motiondata=numpy.loadtxt(config.data['motionpar'])\n",
    "fd=compute_fd(motiondata)\n",
    "dvars=compute_dvars(globalmean)\n",
    "outputdir=os.path.dirname(config.data['motionpar'])\n",
    "config.data['FD']=os.path.join(outputdir,'FD.txt')\n",
    "config.data['DVARS']=os.path.join(outputdir,'DVARS.txt')\n",
    "numpy.savetxt(config.data['FD'],fd)\n",
    "numpy.savetxt(config.data['DVARS'],dvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2,ax3)=plt.subplots(3, sharex=True)\n",
    "ax1.plot(globalmean)\n",
    "ax1.set_title('Mean global signal for in-mask voxels')\n",
    "\n",
    "ax2.plot(fd)\n",
    "ax2.set_title('Framewise Displacement')\n",
    "\n",
    "ax3.plot(dvars)\n",
    "ax3.set_title('DVARS')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corregistration using boundary based registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert os.path.exists(config.data['meanfunc_bbreg_to_t1'])\n",
    "    assert os.path.exists(config.data['meanfunc_bbreg_to_t1_mat'])\n",
    "    assert os.path.exists(config.data['meanfunc_bbreg_to_t1_invmat'])\n",
    "    assert os.path.exists(config.data['meanfunc_unwarped_bbreg_to_t1'])\n",
    "    assert not rerun_analyses\n",
    "except:\n",
    "    epi_reg = mem.cache(fsl.EpiReg)\n",
    "    epi_reg_results=epi_reg(epi=config.data['meanfunc'],\n",
    "                              t1_head=config.data['T1_bc'],\n",
    "                              t1_brain=config.data['T1_brain'],\n",
    "                              fmap=config.data['fieldmap'],\n",
    "                              fmapmag=config.data['fieldmap-mag'],\n",
    "                              fmapmagbrain=config.data['fieldmap-mag_brain'],\n",
    "                              echospacing=2.6/10000.0,\n",
    "                              pedir='y',\n",
    "                              out_base=\"epi2struct\")\n",
    "    \n",
    "    outputdir=os.path.dirname(config.data['meanfunc'])\n",
    "    config.data['meanfunc_bbreg_to_t1']=os.path.join(outputdir, \"epi_bbreg_to_t1.nii.gz\")\n",
    "    config.data['meanfunc_bbreg_to_t1_mat']=os.path.join(outputdir, \"epi_bbreg_to_t1.mat\")\n",
    "    config.data['meanfunc_bbreg_to_t1_invmat']=os.path.join(outputdir, \"epi_bbreg_to_t1_inv.mat\")\n",
    "    \n",
    "    shutil.copy(epi_reg_results.outputs.out_file, config.data['meanfunc_bbreg_to_t1'])\n",
    "    shutil.copy(epi_reg_results.outputs.epi2str_mat , config.data['meanfunc_bbreg_to_t1_mat'])\n",
    "    shutil.copy(epi_reg_results.outputs.epi2str_inv , config.data['meanfunc_bbreg_to_t1_invmat'])\n",
    "    print(epi_reg_results.outputs)\n",
    "    config.data['meanfunc_unwarped_bbreg_to_t1']=os.path.join(outputdir,'epi_unwarped_bbreg_to_t1.nii.gz')\n",
    "    \n",
    "    flirt=mem.cache(fsl.FLIRT)\n",
    "    config.data['meanfunc_unwarped_bbref_to_t1']=os.path.join(outputdir,'epi_unwarped_bbreg_to_t1.nii.gz')\n",
    "    flirt_results=flirt(in_file=config.data['meanfunc_unwarped'],\n",
    "                        out_file=config.data['meanfunc_unwarped_bbreg_to_t1'],\n",
    "                        apply_xfm=True,in_matrix_file=config.data['meanfunc_bbreg_to_t1_mat'],\n",
    "                        reference=config.data['meanfunc_unwarped'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert os.path.exists(config.data['T1_wmseg_mni'])\n",
    "    assert not rerun_analyses\n",
    "except:\n",
    "    T1_to_MNI_warp = mem.cache(ants.ApplyTransforms)\n",
    "    T1wm_to_MNI_warp_results = T1_to_MNI_warp(input_image = config.data['T1_wmseg'],\n",
    "                reference_image = os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz'),\n",
    "                interpolation = \"NearestNeighbor\",\n",
    "                transforms = config.data['T1_mni_warp'])\n",
    "\n",
    "    output_dir=os.path.dirname(config.data['T1_wmseg'])\n",
    "    config.data['T1_wmseg_mni']=os.path.join(output_dir,os.path.basename(T1wm_to_MNI_warp_results.outputs.output_image))\n",
    "    shutil.copy(T1wm_to_MNI_warp_results.outputs.output_image,config.data['T1_wmseg_mni'])\n",
    "\n",
    "fig = nilearn.plotting.plot_anat(os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz'))\n",
    "fig.add_contours(config.data['T1_wmseg_mni'], levels=[.5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perform spatial smoothing using Gaussian kernel. Can you spot the difference between top and bottom rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert os.path.exists(config.data['func_mcf_smooth'])\n",
    "    assert not rerun_analyses\n",
    "except:\n",
    "    smooth = mem.cache(fsl.utils.Smooth)\n",
    "    smooth_results = smooth(fwhm=2.5,\n",
    "                            in_file=config.data['func_mcf'],\n",
    "                            output_type = \"NIFTI\")\n",
    "    output_dir=os.path.dirname(config.data['func_mcf'])\n",
    "    config.data['func_mcf_smooth']=os.path.join(output_dir,\n",
    "                        os.path.basename(smooth_results.outputs.smoothed_file))\n",
    "    shutil.copy(smooth_results.outputs.smoothed_file,config.data['func_mcf_smooth'])\n",
    "\n",
    "nilearn.plotting.plot_epi(config.data['meanfunc'],\n",
    "                              colorbar=True, vmin=410, vmax=21000)\n",
    "nilearn.plotting.plot_epi(nilearn.image.mean_img(config.data['func_mcf_smooth']),\n",
    "                              colorbar=True, vmin=410, vmax=21000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High pass filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-pass filtering - this will take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert os.path.exists(config.data['func_mcf_smooth_hpf_rescaled'])\n",
    "    assert not rerun_analyses\n",
    "except:\n",
    "\n",
    "    hpfilt = mem.cache(fsl.maths.TemporalFilter)\n",
    "    TR = 1.16\n",
    "    hpfilt_results = hpfilt(highpass_sigma = 100/(2*TR),\n",
    "                            in_file=config.data['func_mcf_smooth'],\n",
    "                            output_type = \"NIFTI\")\n",
    "\n",
    "    mean = mem.cache(fsl.maths.MeanImage)\n",
    "    mean_results = mean(in_file = config.data['func_mcf_smooth'])\n",
    "\n",
    "    rescale = mem.cache(fsl.maths.BinaryMaths)\n",
    "    rescale_results = rescale(in_file=hpfilt_results.outputs.out_file,\n",
    "                              operand_file = mean_results.outputs.out_file,\n",
    "                              operation = \"add\",\n",
    "                              output_type = \"NIFTI\")\n",
    "    output_dir=os.path.dirname(config.data['func_mcf_smooth'])\n",
    "    config.data['func_mcf_smooth_hpf_rescaled']=os.path.join(output_dir,\n",
    "                                    os.path.basename(rescale_results.outputs.out_file))\n",
    "    shutil.copy(rescale_results.outputs.out_file,config.data['func_mcf_smooth_hpf_rescaled'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(config.data,open(config.datadict,'wb'))"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
