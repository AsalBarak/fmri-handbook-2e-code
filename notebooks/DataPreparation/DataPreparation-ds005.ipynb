{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates the processed data for dataset ds005, which is used in some of the chapter-specific notebooks from Poldrack, Mumford, and Nichols' _Handbook of fMRI Data Analysis (2nd Edition)_.  This also provides an example of using the nipype workflow mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using base dir: /home/vagrant/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/miniconda3/lib/python3.5/importlib/_bootstrap_external.py:903: FutureWarning: Module nipy.labs.utils.routines deprecated, will be removed\n",
      "  _imp.create_dynamic, spec)\n",
      "/home/vagrant/miniconda3/lib/python3.5/site-packages/nipype/interfaces/nipy/model.py:18: FutureWarning: Module nipy.labs.glm deprecated, will be removed. Please use nipy.modalities.fmri.glm instead.\n",
      "  import nipy.labs.glm.glm as GLM\n"
     ]
    }
   ],
   "source": [
    "import os, errno, sys,shutil\n",
    "import json\n",
    "\n",
    "\n",
    "from fmrihandbook.utils.config import Config\n",
    "\n",
    "config=Config()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from nipype.interfaces import fsl, nipy, ants\n",
    "import nibabel\n",
    "import numpy\n",
    "import nilearn.plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import fmrihandbook.utils\n",
    "from fmrihandbook.utils.compute_fd_dvars import compute_fd,compute_dvars\n",
    "import pickle\n",
    "from fmrihandbook.utils.get_data import get_data\n",
    "\n",
    "import nipype.interfaces.io as nio           # Data i/o\n",
    "import nipype.pipeline.engine as pe          # pypeline engine\n",
    "import nipype.algorithms.modelgen as model   # model specification\n",
    "from nipype.interfaces.base import Bunch\n",
    "import glob\n",
    "import nipype.interfaces.utility as niu\n",
    "from nipype.interfaces.c3 import C3dAffineTool\n",
    "from nipype.interfaces.utility import Merge, IdentityInterface\n",
    "\n",
    "\n",
    "rerun_analyses=False  # set to true to force rerun of everything\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading ds005 from AWS...\n"
     ]
    }
   ],
   "source": [
    "config.data=get_data('ds005')\n",
    "print(config.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "subcodes=[os.path.basename(i) for i in glob.glob(os.path.join(config.data['ds005']['datadir'],'sub-*'))]\n",
    "subcodes.sort()\n",
    "#subcodes=subcodes[:1]\n",
    "print(subcodes)\n",
    "\n",
    "# Map field names to individual subject runs.\n",
    "info = dict(func=[['subject_id','subject_id','runcode']],\n",
    "            anat=[['subject_id', 'subject_id']])\n",
    "\n",
    "infosource = pe.Node(interface=niu.IdentityInterface(fields=['subject_id']), name=\"infosource\")\n",
    "\n",
    "# this builds off of example at http://www.mit.edu/~satra/nipype-nightly/users/examples/fmri_ants_openfmri.html\n",
    "\n",
    "infosource.iterables = ('subject_id', subcodes)\n",
    "\n",
    "runinfo = pe.Node(interface=niu.IdentityInterface(fields=['runcode']), name=\"runinfo\")\n",
    "\n",
    "runinfo.iterables = ('runcode',['1','2','3'])\n",
    "\n",
    "\n",
    "datasource_anat = pe.Node(interface=nio.DataGrabber(infields=['subject_id'],\n",
    "                    outfields=['anat']),\n",
    "                    name = 'datasource_anat')\n",
    "\n",
    "datasource_anat.inputs.base_directory = config.data['ds005']['datadir']\n",
    "\n",
    "datasource_anat.inputs.template = '%s/anat/%s_T1w.nii.gz'\n",
    "\n",
    "\n",
    "datasource_anat.inputs.template_args = dict(anat=[['subject_id','subject_id']])\n",
    "\n",
    "datasource_anat.inputs.sort_filelist = True\n",
    "\n",
    "                     \n",
    "datasource_func = pe.Node(interface=nio.DataGrabber(infields=['subject_id','runcode'],\n",
    "                    outfields=['func']),\n",
    "                    name = 'datasource_func')\n",
    "\n",
    "datasource_func.inputs.base_directory = config.data['ds005']['datadir']\n",
    "\n",
    "datasource_func.inputs.template = '%s/func/%s_task-mixedgamblestask_run-0%s_bold.nii.gz'\n",
    "\n",
    "datasource_func.inputs.template_args = dict(func=[['subject_id','subject_id','runcode']])\n",
    "\n",
    "datasource_func.inputs.sort_filelist = True\n",
    "\n",
    "                                                 \n",
    "preprocessing = pe.Workflow(name=\"preprocessing\")\n",
    "\n",
    "try:\n",
    "    workdir='/Users/poldrack/data_unsynced/fmri-handbook-2e-data/ds005/nipype_workdir'\n",
    "    assert os.path.exists(workdir)\n",
    "except:\n",
    "    workdir=os.path.join(config.data['ds005']['datadir'],'nipype_workdir')\n",
    "    if not os.path.exists(workdir):\n",
    "        os.mkdir(workdir)\n",
    "\n",
    "preprocessing.base_dir = workdir\n",
    "\n",
    "preprocessing.connect(infosource,'subject_id',datasource_anat,'subject_id')\n",
    "\n",
    "preprocessing.connect(infosource,'subject_id',datasource_func,'subject_id')\n",
    "preprocessing.connect(runinfo,'runcode',datasource_func,'runcode')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias field correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bfc = pe.Node(interface=ants.N4BiasFieldCorrection(), name=\"bfc\")\n",
    "bfc.inputs.dimension = 3\n",
    "bfc.inputs.save_bias = True\n",
    "\n",
    "preprocessing.connect(datasource_anat, 'anat', bfc, 'input_image')\n",
    "\n",
    "datasink = pe.Node(nio.DataSink(), name='datasink')\n",
    "\n",
    "# Save the relevant data into an output directory\n",
    "datasink.inputs.base_directory = os.path.join(config.data['ds005']['datadir'],'derivatives')\n",
    "if not os.path.exists(datasink.inputs.base_directory):\n",
    "    os.mkdir(datasink.inputs.base_directory)\n",
    "\n",
    "\n",
    "preprocessing.connect(bfc, 'bias_image', datasink, 'bfc.bias')\n",
    "preprocessing.connect(bfc, 'output_image', datasink, 'bfc.output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain extraction using BET###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet_struct=pe.Node(interface=fsl.BET(), name=\"bet_struct\")\n",
    "bet_struct.inputs.reduce_bias=True\n",
    "bet_struct.inputs.frac=0.4\n",
    "\n",
    "preprocessing.connect(bfc,'output_image',bet_struct,'in_file')\n",
    "preprocessing.connect(bet_struct, 'out_file', datasink, 'bet.output')\n",
    "preprocessing.connect(bet_struct, 'mask_file', datasink, 'bet.mask')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation using FAST\n",
    "\n",
    "Do this to obtain the white matter mask, which we need for BBR registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast=pe.Node(interface=fsl.FAST(), name=\"fast\")\n",
    "\n",
    "preprocessing.connect(bet_struct,'out_file',fast,'in_files')\n",
    "preprocessing.connect(fast, 'partial_volume_files', datasink, 'fast.pvefiles')\n",
    "preprocessing.connect(fast, 'tissue_class_map', datasink, 'fast.seg')\n",
    "\n",
    "binarize = pe.Node(fsl.ImageMaths(op_string='-nan -thr 0.5 -bin'),\n",
    "                   name='binarize')\n",
    "pickindex = lambda x, i: x[i]\n",
    "preprocessing.connect(fast, ('partial_volume_files', pickindex, 2),\n",
    "                 binarize, 'in_file')\n",
    "preprocessing.connect(binarize, 'out_file', datasink, 'fast.wmseg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial normalization using ANTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "antsreg=pe.Node(interface=ants.Registration(), name=\"antsreg\")\n",
    "antsreg.inputs.fixed_image = os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz')\n",
    "antsreg.inputs.transforms = ['Translation', 'Rigid', 'Affine', 'SyN']\n",
    "antsreg.inputs.transform_parameters = [(0.1,), (0.1,), (0.1,), (0.2, 3.0, 0.0)]\n",
    "antsreg.inputs.number_of_iterations = ([[10, 10, 10]]*3 +\n",
    "                [[1, 5, 3]])\n",
    "antsreg.inputs.dimension = 3\n",
    "antsreg.inputs.write_composite_transform = True\n",
    "antsreg.inputs.metric = ['Mattes'] * 3 + [['Mattes', 'CC']]\n",
    "antsreg.inputs.metric_weight = [1] * 3 + [[0.5, 0.5]]\n",
    "antsreg.inputs.radius_or_number_of_bins = [32] * 3 + [[32, 4]]\n",
    "antsreg.inputs.sampling_strategy = ['Regular'] * 3 + [[None, None]]\n",
    "antsreg.inputs.sampling_percentage = [0.3] * 3 + [[None, None]]\n",
    "antsreg.inputs.convergence_threshold = [1.e-8] * 3 + [-0.01]\n",
    "antsreg.inputs.convergence_window_size = [20] * 3 + [5]\n",
    "antsreg.inputs.smoothing_sigmas = [[4, 2, 1]] * 3 + [[1, 0.5, 0]]\n",
    "antsreg.inputs.sigma_units = ['vox'] * 4\n",
    "antsreg.inputs.shrink_factors = [[6, 4, 2]] + [[3, 2, 1]]*2 + [[4, 2, 1]]\n",
    "antsreg.inputs.use_estimate_learning_rate_once = [True] * 4\n",
    "antsreg.inputs.use_histogram_matching = [False] * 3 + [True]\n",
    "antsreg.inputs.initial_moving_transform_com = True\n",
    "antsreg.inputs.output_warped_image = True\n",
    "\n",
    "preprocessing.connect(bet_struct,'out_file',antsreg,'moving_image')\n",
    "preprocessing.connect(antsreg, 'warped_image', datasink, 'ants.warped_image')\n",
    "preprocessing.connect(antsreg, 'composite_transform', datasink, 'ants.composite_transform')\n",
    "preprocessing.connect(antsreg, 'inverse_composite_transform', datasink, 'ants.inverse_composite_transform')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get linear warp of anatomy to MNI space, for comparison to nonlinear\n",
    "linearMNI=pe.Node(fsl.FLIRT(), name='linearMNI')\n",
    "linearMNI.inputs.reference=os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz')\n",
    "linearMNI.inputs.dof=12\n",
    "\n",
    "preprocessing.connect(bet_struct,'out_file',linearMNI,'in_file')\n",
    "\n",
    "preprocessing.connect(linearMNI,'out_file',datasink,'affine.out_file')\n",
    "preprocessing.connect(linearMNI,'out_matrix_file',datasink,'affine.matrix')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional preprocessing\n",
    "\n",
    "### Motion correction using MCFLIRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mcflirt=pe.Node(interface=fsl.MCFLIRT(), name=\"mcflirt\")\n",
    "mcflirt.inputs.save_plots=True\n",
    "mcflirt.inputs.mean_vol=True\n",
    "\n",
    "preprocessing.connect(datasource_func, 'func', mcflirt, 'in_file')\n",
    "\n",
    "preprocessing.connect(mcflirt, 'out_file', datasink, 'mcflirt.out_file')\n",
    "preprocessing.connect(mcflirt, 'par_file', datasink, 'mcflirt.par')\n",
    "preprocessing.connect(mcflirt, 'mean_img', datasink, 'mcflirt.mean')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make links for the mean functional image and the motion parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain extraction\n",
    "\n",
    "Use FSL's BET to obtain the brain mask for the functional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bet_func=pe.Node(interface=fsl.BET(), name=\"bet_func\")\n",
    "\n",
    "bet_func.inputs.functional=True\n",
    "bet_func.inputs.mask=True\n",
    "\n",
    "preprocessing.connect(mcflirt, 'out_file', bet_func, 'in_file')\n",
    "\n",
    "preprocessing.connect(bet_func, 'out_file', datasink, 'betfunc.out_file')\n",
    "preprocessing.connect(bet_func, 'mask_file', datasink, 'betfunc.mask_file')\n",
    "\n",
    "meanbetfunc=pe.Node(interface=fsl.MeanImage(), name=\"meanbetfunc\")\n",
    "preprocessing.connect(bet_func,'out_file',meanbetfunc,'in_file')\n",
    "preprocessing.connect(meanbetfunc, 'out_file', datasink, 'betfunc.mean_file')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BBR registration of functional to structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean2anat = pe.Node(fsl.FLIRT(), name='mean2anat')\n",
    "mean2anat.inputs.dof = 6\n",
    "preprocessing.connect(meanbetfunc, 'out_file', mean2anat, 'in_file')\n",
    "preprocessing.connect(bet_struct, 'out_file', mean2anat, 'reference')\n",
    "\n",
    "mean2anatbbr = pe.Node(fsl.FLIRT(), name='mean2anatbbr')\n",
    "mean2anatbbr.inputs.dof = 6\n",
    "mean2anatbbr.inputs.cost = 'bbr'\n",
    "mean2anatbbr.inputs.schedule = os.path.join(os.getenv('FSLDIR'),\n",
    "                                            'etc/flirtsch/bbr.sch')\n",
    "\n",
    "preprocessing.connect(meanbetfunc, 'out_file', mean2anatbbr, 'in_file')\n",
    "preprocessing.connect(binarize, 'out_file', mean2anatbbr, 'wm_seg')\n",
    "preprocessing.connect(bet_struct, 'out_file', mean2anatbbr, 'reference')\n",
    "preprocessing.connect(mean2anat, 'out_matrix_file',\n",
    "                 mean2anatbbr, 'in_matrix_file')\n",
    "\n",
    "preprocessing.connect(mean2anat, 'out_matrix_file', datasink, 'mean2anat.out_matrix')\n",
    "\n",
    "preprocessing.connect(mean2anatbbr, 'out_matrix_file', datasink, 'bbr.out_matrix')\n",
    "preprocessing.connect(mean2anatbbr, 'out_file', datasink, 'bbr.out_file')\n",
    "\n",
    "\n",
    "# convert BBR matrix to ITK for ANTS\n",
    "\n",
    "convert2itk = pe.Node(C3dAffineTool(),\n",
    "                      name='convert2itk')\n",
    "convert2itk.inputs.fsl2ras = True\n",
    "convert2itk.inputs.itk_transform = True\n",
    "preprocessing.connect(mean2anatbbr, 'out_matrix_file', convert2itk, 'transform_file')\n",
    "preprocessing.connect(meanbetfunc, 'out_file', convert2itk, 'source_file')\n",
    "preprocessing.connect(bet_struct, 'out_file', convert2itk, 'reference_file')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate the affine and ants transforms into a list\n",
    "\n",
    "pickfirst = lambda x: x[0]\n",
    "\n",
    "merge = pe.Node(Merge(2),  name='mergexfm')\n",
    "preprocessing.connect(convert2itk, 'itk_transform', merge, 'in2')\n",
    "preprocessing.connect(antsreg, 'composite_transform', merge, 'in1')\n",
    "\n",
    "warpmean = pe.Node(ants.ApplyTransforms(), name='warpmean')\n",
    "warpmean.inputs.input_image_type = 0\n",
    "warpmean.inputs.interpolation = 'Linear'\n",
    "warpmean.inputs.invert_transform_flags = [False] #[False, False]\n",
    "warpmean.inputs.terminal_output = 'file'\n",
    "warpmean.inputs.args = '--float'\n",
    "warpmean.inputs.reference_image = os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz')\n",
    "\n",
    "\n",
    "preprocessing.connect(mean2anatbbr, 'out_file', warpmean, 'input_image')\n",
    "preprocessing.connect(antsreg, 'composite_transform', warpmean, 'transforms')\n",
    "#preprocessing.connect(merge, 'out', warpmean, 'transforms')\n",
    "preprocessing.connect(warpmean,'output_image',datasink,'ants.warped_mean')\n",
    "\n",
    "\n",
    "# high pass filtering\n",
    "\n",
    "hpfilt=pe.Node(interface=fsl.maths.TemporalFilter(),name='hpfilt')\n",
    "TR=2.0\n",
    "hpf_cutoff=80.\n",
    "\n",
    "hpfilt.inputs.highpass_sigma = hpf_cutoff/(2*TR)\n",
    "preprocessing.connect(bet_func, 'out_file',hpfilt,'in_file')\n",
    "\n",
    "\n",
    "preprocessing.connect(hpfilt,'out_file',datasink,'hpfilt')\n",
    "\n",
    "rescale = pe.Node(interface=fsl.maths.BinaryMaths(),name='rescale')\n",
    "rescale.inputs.operation = \"add\"\n",
    "\n",
    "preprocessing.connect(hpfilt,'out_file',rescale,'in_file')\n",
    "preprocessing.connect(meanbetfunc, 'out_file',rescale,'operand_file')\n",
    "\n",
    "preprocessing.connect(rescale,'out_file',datasink,'rescaled')\n",
    "\n",
    "# spatial smoothing using FSL's susan or regular gaussian smoothing\n",
    "\n",
    "use_SUSAN=False\n",
    "\n",
    "if use_SUSAN:\n",
    "    medianval = pe.MapNode(interface=fsl.ImageStats(op_string='-k %s -p 50'),\n",
    "                           iterfield=['in_file','mask_file'],\n",
    "                           name='medianval')\n",
    "    preprocessing.connect(mcflirt, 'out_file', medianval, 'in_file')\n",
    "    preprocessing.connect(bet_func, 'mask_file', medianval, 'mask_file')\n",
    "\n",
    "    smooth = pe.MapNode(interface=fsl.SUSAN(),\n",
    "                        iterfield=['in_file', 'brightness_threshold'],\n",
    "                        name='smooth')\n",
    "    def getbtthresh(medianvals):\n",
    "        return [0.75 * val for val in medianvals]\n",
    "\n",
    "    preprocessing.connect(medianval, ('out_stat', getbtthresh), smooth, 'brightness_threshold')\n",
    "else:\n",
    "    smooth=pe.Node(interface=fsl.utils.Smooth(), name=\"smooth\")\n",
    "\n",
    "smooth.inputs.fwhm=6\n",
    "\n",
    "# Turn off smoothing for now - we will do it later on the constrast images\n",
    "#preprocessing.connect(rescale,'out_file',smooth,'in_file')\n",
    "#preprocessing.connect(smooth,'smoothed_file',datasink,'smooth')\n",
    "\n",
    "\n",
    "\n",
    "graph=preprocessing.run() #plugin='MultiProc', plugin_args={'n_procs' : 2})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration check\n",
    "#### Show overlays of highres-template and functional-highres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regcheck=False\n",
    "if regcheck:\n",
    "  for subcode in subcodes:\n",
    "    antsregfile=os.path.join(config.data['ds005']['datadir'],\n",
    "                             'derivatives/ants/warped_image/_subject_id_%s/transform_Warped.nii.gz'%subcode)\n",
    "    if os.path.exists(antsregfile):\n",
    "        mask_display=nilearn.plotting.plot_epi(antsregfile,cmap='gray')\n",
    "        mask_display.add_edges(os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# linear warp\n",
    "if regcheck:\n",
    "  for subcode in subcodes:\n",
    "    antsregfile=os.path.join(config.data['ds005']['datadir'],\n",
    "                             'derivatives/affine/out_file/_subject_id_%s/%s_T1w_corrected_brain_flirt.nii.gz'%(subcode,subcode))\n",
    "    if os.path.exists(antsregfile):\n",
    "        mask_display=nilearn.plotting.plot_epi(antsregfile,cmap='gray')\n",
    "        mask_display.add_edges(os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if regcheck:\n",
    "  for subcode in subcodes:\n",
    "    antsregfile=os.path.join(config.data['ds005']['datadir'],\n",
    "                             'derivatives/bet/output/_subject_id_%s/%s_T1w_corrected_brain.nii.gz'%(subcode,subcode))\n",
    "    if os.path.exists(antsregfile):\n",
    "        mask_display=nilearn.plotting.plot_epi(antsregfile,cmap='gray')\n",
    "        mask_display.add_edges(os.path.join(config.data['ds005']['datadir'],\n",
    "            'derivatives/bbr/out_file/_runcode_1/_subject_id_%s/%s_task-mixedgamblestask_run-01_bold_mcf_brain_mean_flirt.nii.gz'%(subcode,subcode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# warped functional to MNI\n",
    "if regcheck:\n",
    "  for subcode in subcodes:\n",
    "    antsregfile=os.path.join(config.data['ds005']['datadir'],\n",
    "       'derivatives/ants/warped_mean/_runcode_1/_subject_id_%s/%s_task-mixedgamblestask_run-01_bold_mcf_brain_mean_flirt_trans.nii.gz'%(subcode,subcode))\n",
    "    if os.path.exists(antsregfile):\n",
    "        mask_display=nilearn.plotting.plot_epi(antsregfile,cmap='gray')\n",
    "        mask_display.add_edges(os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA for fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load motion data and compute FD/DVARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_qa=False\n",
    "if do_qa:\n",
    "    fd={}\n",
    "    dvars={}\n",
    "    for subcode in subcodes:\n",
    "        fd[subcode]={}\n",
    "        dvars[subcode]={}\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for runcode in range(1,4):\n",
    "            motfile=os.path.join(config.data['ds005']['datadir'],\n",
    "                'derivatives/mcflirt/par/_runcode_%d/_subject_id_%s/%s_task-mixedgamblestask_run-%02d_bold_mcf.nii.gz.par'%(runcode,subcode,subcode,runcode))\n",
    "            outdir=os.path.dirname(motfile)\n",
    "            try:\n",
    "                fd[subcode][runcode]=numpy.loadtxt(os.path.join(outdir,'fd.txt'))\n",
    "                dvars[subcode][runcode]=numpy.loadtxt(os.path.join(outdir,'dvars.txt'))\n",
    "            except:\n",
    "                motiondata=numpy.loadtxt(motfile)\n",
    "                fd[subcode][runcode]=compute_fd(motiondata)\n",
    "                meanfile=os.path.join(config.data['ds005']['datadir'],\n",
    "                    'derivatives/betfunc/out_file/_runcode_%d/_subject_id_%s/%s_task-mixedgamblestask_run-%02d_bold_mcf_brain.nii.gz'%(runcode,subcode,subcode,runcode))\n",
    "                meandata=nibabel.load(meanfile)\n",
    "                mask=nibabel.load(os.path.join(config.data['ds005']['datadir'],\n",
    "                    'derivatives/betfunc/mask_file/_runcode_%d/_subject_id_%s/%s_task-mixedgamblestask_run-%02d_bold_mcf_brain_mask.nii.gz'%(runcode,subcode,subcode,runcode)))\n",
    "                masker=NiftiMasker(mask_img=mask)\n",
    "                maskdata=masker.fit_transform(meandata)\n",
    "                globalmean=numpy.mean(maskdata,0)\n",
    "                dvars[subcode][runcode]=compute_dvars(globalmean)\n",
    "                numpy.savetxt(os.path.join(outdir,'fd.txt'),fd[subcode][runcode])\n",
    "                numpy.savetxt(os.path.join(outdir,'dvars.txt'),dvars[subcode][runcode])\n",
    "\n",
    "            plt.subplot(1,3,runcode)\n",
    "            plt.plot(fd[subcode][runcode])\n",
    "            #plt.plot(dvars[subcode][runcode]/100,color='r')\n",
    "\n",
    "            plt.axis([0,250,0,2.0])\n",
    "            plt.title('%s: run %d'%(subcode,runcode))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-level modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firstlevel = pe.Workflow(name=\"firstlevel\")\n",
    "\n",
    "\n",
    "firstlevel.base_dir = workdir\n",
    "\n",
    "\n",
    "                     \n",
    "datasource_func = pe.Node(interface=nio.DataGrabber(infields=['subject_id','runcode'],\n",
    "                    outfields=['func']),\n",
    "                    name = 'datasource_func')\n",
    "\n",
    "datasource_func.inputs.base_directory = config.data['ds005']['datadir']\n",
    "\n",
    "datasource_func.inputs.template = 'derivatives/rescaled/_runcode_%s/_subject_id_%s/%s_task-mixedgamblestask_run-0%s_bold_mcf_brain_filt_maths.nii.gz'\n",
    "\n",
    "datasource_func.inputs.template_args = dict(func=[['runcode','subject_id','subject_id','runcode']])\n",
    "\n",
    "datasource_func.inputs.sort_filelist = True\n",
    "\n",
    "\n",
    "firstlevel.connect(infosource,'subject_id',datasource_anat,'subject_id')\n",
    "firstlevel.connect(infosource,'subject_id',datasource_func,'subject_id')\n",
    "firstlevel.connect(runinfo,'runcode',datasource_func,'runcode')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creat a function that can read all of the onsets and motion files\n",
    "# and generate the appropriate structure\n",
    "\n",
    "def get_onsets(subject_id,runnum):\n",
    "    from fmrihandbook.utils.config import Config\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    import numpy\n",
    "    import os,json\n",
    "\n",
    "\n",
    "    config=Config()\n",
    "    \n",
    "    regressors=[]\n",
    "    regressor_names=[]\n",
    "    fdfile=os.path.join(config.data['ds005']['datadir'],'derivatives/mcflirt/par/_runcode_%s/_subject_id_%s/fd.txt'%(runnum,subject_id))\n",
    "    regressors.append(numpy.loadtxt(fdfile))\n",
    "    regressor_names.append('fd')\n",
    "    \n",
    "    dvarsfile=os.path.join(config.data['ds005']['datadir'],'derivatives/mcflirt/par/_runcode_%s/_subject_id_%s/dvars.txt'%(runnum,subject_id))\n",
    "    regressors.append(numpy.loadtxt(dvarsfile))\n",
    "    regressor_names.append('dvars')\n",
    "\n",
    "    motfile=os.path.join(config.data['ds005']['datadir'],\n",
    "                'derivatives/mcflirt/par/_runcode_%s/_subject_id_%s/%s_task-mixedgamblestask_run-0%s_bold_mcf.nii.gz.par'%(runnum,subject_id,subject_id,runnum))\n",
    "    motpars=numpy.loadtxt(motfile)\n",
    "    for i in range(motpars.shape[1]):\n",
    "        regressors.append(motpars[:,i])\n",
    "        regressor_names.append('motpar%d'%int(i+1))\n",
    "        td=numpy.zeros(motpars.shape[0])\n",
    "        td[1:]=motpars[1:,i] -motpars[:-1,i]\n",
    "        regressors.append(td)\n",
    "        regressor_names.append('disp%d'%int(i+1))\n",
    "\n",
    "                         \n",
    "    onsfile=os.path.join(config.data['ds005']['datadir'],'ds005_onsets.json')\n",
    "    with open(onsfile, 'r') as f:\n",
    "        ons = json.load(f) \n",
    "    conditions=['task','param-gain','param-loss','param-rt']\n",
    "    info=ons[subject_id]['run-0%s'%runnum]\n",
    "    onsets=[]\n",
    "    durations=[]\n",
    "    amplitudes=[]\n",
    "    for c in conditions:\n",
    "        onsets.append(info[c]['onset'])\n",
    "        durations.append(info[c]['duration'])\n",
    "        amplitudes.append(info[c]['weight'])\n",
    "    \n",
    "    info = [Bunch(conditions=conditions,\n",
    "              onsets=onsets,\n",
    "              durations=durations,\n",
    "                amplitudes=amplitudes,\n",
    "                 regressors=regressors,\n",
    "                 regressor_names=regressor_names)]\n",
    "    return info\n",
    "\n",
    "getonsets = pe.Node(niu.Function(input_names=['subject_id','runnum'],\n",
    "                output_names=['info'],\n",
    "                function=get_onsets),\n",
    "                name='getonsets')\n",
    "\n",
    "firstlevel.connect(infosource,'subject_id',getonsets,'subject_id')\n",
    "firstlevel.connect(runinfo,'runcode',getonsets,'runnum')\n",
    "\n",
    "specifymodel=pe.Node(interface=model.SpecifyModel(),name='specifymodel')\n",
    "\n",
    "specifymodel.inputs.input_units = 'secs'\n",
    "#specifymodel.inputs.functional_runs = preprocessed_epi\n",
    "specifymodel.inputs.time_repetition = 2.0\n",
    "specifymodel.inputs.high_pass_filter_cutoff = hpf_cutoff\n",
    "#s.inputs.subject_info = info\n",
    "firstlevel.connect(getonsets,'info',specifymodel,'subject_info')\n",
    "firstlevel.connect(datasource_func,'func',specifymodel,'functional_runs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contrasts=[['task>Baseline','T', \n",
    "            ['task'],[1]],\n",
    "           ['param-gain','T', \n",
    "            ['param-gain'],[1]],\n",
    "           ['param-loss-neg','T', \n",
    "            ['param-loss'],[-1]],\n",
    "           ['param-rt','T', \n",
    "            ['param-rt'],[1]]]\n",
    "           \n",
    "level1design = pe.Node(interface=fsl.model.Level1Design(),name='level1design')\n",
    "level1design.inputs.interscan_interval =2.0\n",
    "level1design.inputs.bases = {'dgamma':{'derivs': True}}\n",
    "level1design.inputs.model_serial_correlations=True\n",
    "level1design.inputs.contrasts=contrasts\n",
    "\n",
    "firstlevel.connect(specifymodel,'session_info',level1design,'session_info')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelgen = pe.Node(interface=fsl.model.FEATModel(),name='modelgen')\n",
    "firstlevel.connect(level1design,'fsf_files',modelgen,'fsf_file')\n",
    "firstlevel.connect(level1design,'ev_files',modelgen,'ev_files')\n",
    "\n",
    "filmgls= pe.Node(interface=fsl.FILMGLS(),name='filmgls')\n",
    "\n",
    "filmgls.inputs.autocorr_noestimate = True\n",
    "firstlevel.connect(datasource_func,'func',filmgls,'in_file')\n",
    "firstlevel.connect(modelgen,'design_file',filmgls,'design_file')\n",
    "firstlevel.connect(modelgen,'con_file',filmgls,'tcon_file')\n",
    "\n",
    "firstlevel.connect(filmgls,'param_estimates',datasink,'filmgls.param_estimates')\n",
    "firstlevel.connect(filmgls,'sigmasquareds',datasink,'filmgls.sigmasquareds')\n",
    "firstlevel.connect(filmgls,'copes',datasink,'filmgls.copes')\n",
    "firstlevel.connect(filmgls,'varcopes',datasink,'filmgls.varcopes')\n",
    "firstlevel.connect(filmgls,'dof_file',datasink,'filmgls.dof_file')\n",
    "firstlevel.connect(filmgls,'tstats',datasink,'filmgls.tstats')\n",
    "firstlevel.connect(filmgls,'zstats',datasink,'filmgls.zstats')\n",
    "\n",
    "\n",
    "firstlevel.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## warp copes and varcopes to MNI space\n",
    "\n",
    "use suboptimal two-step procedure - first apply linear registration from BBR, then apply nonlinear from ANTS\n",
    "\n",
    "this is necesary because of errors getting FSL mat file into ANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasource_stat = pe.Node(interface=nio.DataGrabber(infields=['subject_id','runcode'],\n",
    "                    outfields=['stats']),\n",
    "                    name = 'datasource_stat')\n",
    "\n",
    "datasource_stat.inputs.base_directory = config.data['ds005']['datadir']\n",
    "datasource_stat.inputs.template = 'derivatives/filmgls/*copes/_runcode_%s/_subject_id_%s/*.nii.gz'\n",
    "datasource_stat.inputs.template_args = dict(stats=[['runcode','subject_id']])\n",
    "datasource_stat.inputs.sort_filelist = True\n",
    "\n",
    "\n",
    "datasource_bbrmat = pe.Node(interface=nio.DataGrabber(infields=['subject_id','runcode'],\n",
    "                    outfields=['bbrmat']),\n",
    "                    name = 'datasource_bbrmat')\n",
    "\n",
    "datasource_bbrmat.inputs.base_directory = config.data['ds005']['datadir']\n",
    "datasource_bbrmat.inputs.template = 'derivatives/bbr/out_matrix/_runcode_%s/_subject_id_%s/*.mat'\n",
    "datasource_bbrmat.inputs.template_args = dict(bbrmat=[['runcode','subject_id']])\n",
    "datasource_bbrmat.inputs.sort_filelist = True\n",
    "\n",
    "datasource_antsreg = pe.Node(interface=nio.DataGrabber(infields=['subject_id','runcode'],\n",
    "                    outfields=['composite_transform']),\n",
    "                    name = 'datasource_antsreg')\n",
    "\n",
    "datasource_antsreg.inputs.base_directory = config.data['ds005']['datadir']\n",
    "datasource_antsreg.inputs.template = 'derivatives/ants/composite_transform/_subject_id_%s/transformComposite.h5'\n",
    "datasource_antsreg.inputs.template_args = dict(composite_transform=[['subject_id']])\n",
    "datasource_antsreg.inputs.sort_filelist = True\n",
    "\n",
    "datasource_linearreg = pe.Node(interface=nio.DataGrabber(infields=['subject_id','runcode'],\n",
    "                    outfields=['matrix']),\n",
    "                    name = 'datasource_linearreg')\n",
    "\n",
    "datasource_linearreg.inputs.base_directory = config.data['ds005']['datadir']\n",
    "datasource_linearreg.inputs.template = 'derivatives/affine/matrix/_subject_id_%s/%s_T1w_corrected_brain_flirt.mat'\n",
    "datasource_linearreg.inputs.template_args = dict(matrix=[['subject_id','subject_id']])\n",
    "datasource_linearreg.inputs.sort_filelist = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "secondlevel = pe.Workflow(name='secondlevel')\n",
    "secondlevel.base_dir = workdir\n",
    "\n",
    "\n",
    "secondlevel.connect(infosource,'subject_id',datasource_bbrmat,'subject_id')\n",
    "secondlevel.connect(infosource,'subject_id',datasource_stat,'subject_id')\n",
    "secondlevel.connect(infosource,'subject_id',datasource_anat,'subject_id')\n",
    "secondlevel.connect(infosource,'subject_id',datasource_antsreg,'subject_id')\n",
    "secondlevel.connect(infosource,'subject_id',datasource_linearreg,'subject_id')\n",
    "secondlevel.connect(infosource,'subject_id',datasource_func,'subject_id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "secondlevel.connect(runinfo,'runcode',datasource_stat,'runcode')\n",
    "secondlevel.connect(runinfo,'runcode',datasource_bbrmat,'runcode')\n",
    "secondlevel.connect(runinfo,'runcode',datasource_anat,'runcode')\n",
    "secondlevel.connect(runinfo,'runcode',datasource_antsreg,'runcode')\n",
    "secondlevel.connect(runinfo,'runcode',datasource_linearreg,'runcode')\n",
    "secondlevel.connect(runinfo,'runcode',datasource_func,'runcode')\n",
    "\n",
    "\n",
    "bbrstats=pe.MapNode(fsl.FLIRT(),name='bbrstats', iterfield=['in_file'])\n",
    "bbrstats.inputs.apply_xfm=True\n",
    "\n",
    "\n",
    "secondlevel.connect(datasource_stat, 'stats', bbrstats,'in_file')\n",
    "secondlevel.connect(datasource_bbrmat, 'bbrmat', bbrstats, 'in_matrix_file')\n",
    "secondlevel.connect(datasource_anat, 'anat', bbrstats,'reference')\n",
    "\n",
    "secondlevel.connect(bbrstats,'out_file',datasink,'bbr.stats')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# warp stats maps using both ANTS and linear registration\n",
    "\n",
    "warpstats = pe.MapNode(ants.ApplyTransforms(), name='warpstats', iterfield=['input_image'])\n",
    "warpstats.inputs.input_image_type = 0\n",
    "warpstats.inputs.interpolation = 'Linear'\n",
    "warpstats.inputs.invert_transform_flags = [False] #[False, False]\n",
    "warpstats.inputs.terminal_output = 'file'\n",
    "warpstats.inputs.args = '--float'\n",
    "warpstats.inputs.reference_image = os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz')\n",
    "\n",
    "#secondlevel.connect(bbrstats, 'out_file', warpstats, 'subject_id')\n",
    "\n",
    "\n",
    "secondlevel.connect(bbrstats, 'out_file', warpstats, 'input_image')\n",
    "secondlevel.connect(datasource_antsreg, 'composite_transform', warpstats, 'transforms')\n",
    "secondlevel.connect(warpstats,'output_image',datasink,'ants.warped_stats')\n",
    "\n",
    "warpstats_linear=pe.MapNode(fsl.FLIRT(),name='warpstats_linear', iterfield=['in_file'])\n",
    "warpstats_linear.inputs.apply_xfm=True\n",
    "warpstats_linear.inputs.reference=os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain.nii.gz')\n",
    "\n",
    "\n",
    "secondlevel.connect(bbrstats, 'out_file', warpstats_linear, 'in_file')\n",
    "secondlevel.connect(datasource_linearreg, 'matrix', warpstats_linear, 'in_matrix_file')\n",
    "secondlevel.connect(warpstats_linear,'out_file',datasink,'affine.warped_stats')\n",
    "\n",
    "\n",
    "secondlevel.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second level analysis - fixed effects across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fixed_fx = pe.Workflow(name='fixedfx')\n",
    "fixed_fx.base_dir = workdir\n",
    "\n",
    "copeinfo = pe.Node(interface=niu.IdentityInterface(fields=['copenum']), name=\"copeinfo\")\n",
    "\n",
    "copeinfo.iterables = ('copenum',[1,2,3,4])\n",
    "\n",
    "regtypes = pe.Node(interface=niu.IdentityInterface(fields=['regtype']), name=\"regtypes\")\n",
    "\n",
    "regtypes.iterables = ('regtype',['ants','affine'])\n",
    "\n",
    "\n",
    "datasource_cope = pe.Node(interface=nio.DataGrabber(infields=['subject_id','copenum','regtype'],\n",
    "                    outfields=['copes']),\n",
    "                    name = 'datasource_cope')\n",
    "\n",
    "datasource_cope.inputs.base_directory = config.data['ds005']['datadir']\n",
    "datasource_cope.inputs.template = 'derivatives/%s/warped_stats/_runcode_*/_subject_id_%s/_warpstats*/cope%d_*.nii.gz'\n",
    "datasource_cope.inputs.template_args = dict(copes=[['regtype','subject_id','copenum']])\n",
    "datasource_cope.inputs.sort_filelist = True\n",
    "\n",
    "datasource_varcope = pe.Node(interface=nio.DataGrabber(infields=['subject_id','copenum','regtype'],\n",
    "                    outfields=['varcopes']),\n",
    "                    name = 'datasource_varcope')\n",
    "\n",
    "datasource_varcope.inputs.base_directory = config.data['ds005']['datadir']\n",
    "datasource_varcope.inputs.template = 'derivatives/%s/warped_stats/_runcode_*/_subject_id_%s/_warpstats*/varcope%d_*.nii.gz'\n",
    "datasource_varcope.inputs.template_args = dict(varcopes=[['regtype','subject_id','copenum']])\n",
    "datasource_varcope.inputs.sort_filelist = True\n",
    "\n",
    "\n",
    "fixed_fx.connect(infosource,'subject_id',datasource_cope,'subject_id')\n",
    "fixed_fx.connect(infosource,'subject_id',datasource_varcope,'subject_id')\n",
    "fixed_fx.connect(copeinfo,'copenum',datasource_cope,'copenum')\n",
    "fixed_fx.connect(copeinfo,'copenum',datasource_varcope,'copenum')\n",
    "fixed_fx.connect(regtypes,'regtype',datasource_cope,'regtype')\n",
    "fixed_fx.connect(regtypes,'regtype',datasource_varcope,'regtype')\n",
    "\n",
    "\n",
    "\n",
    "copemerge    = pe.MapNode(interface=fsl.Merge(dimension='t'),\n",
    "                          iterfield=['in_files'],\n",
    "                          name=\"copemerge\")\n",
    "\n",
    "varcopemerge = pe.MapNode(interface=fsl.Merge(dimension='t'),\n",
    "                       iterfield=['in_files'],\n",
    "                       name=\"varcopemerge\")\n",
    "\n",
    "fixed_fx.connect(datasource_cope,'copes',copemerge,'in_files')\n",
    "fixed_fx.connect(datasource_varcope,'varcopes',varcopemerge,'in_files')\n",
    "\n",
    "level2model = pe.Node(interface=fsl.L2Model(),\n",
    "                      name='l2model')\n",
    "level2model.inputs.num_copes=3\n",
    "\n",
    "\n",
    "flameo = pe.MapNode(interface=fsl.FLAMEO(run_mode='fe'), name=\"flameo\",\n",
    "                    iterfield=['cope_file','var_cope_file'])\n",
    "flameo.inputs.mask_file=os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain_mask.nii.gz')\n",
    "\n",
    "\n",
    "fixed_fx.connect(copemerge,'merged_file',flameo,'cope_file')\n",
    "fixed_fx.connect(varcopemerge,'merged_file',flameo,'var_cope_file')\n",
    "\n",
    "fixed_fx.connect(level2model,'design_mat',flameo,'design_file')\n",
    "fixed_fx.connect(level2model,'design_con',flameo,'t_con_file')\n",
    "fixed_fx.connect(level2model,'design_grp',flameo,'cov_split_file')\n",
    "\n",
    "fixed_fx.connect(flameo,'copes',datasink,'fixedfx.cope')\n",
    "fixed_fx.connect(flameo,'var_copes',datasink,'fixedfx.varcope')\n",
    "\n",
    "\n",
    "fixed_fx.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-level model: across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mixed_fx = pe.Workflow(name='mixedfx')\n",
    "mixed_fx.base_dir = workdir\n",
    "\n",
    "smoothlevels = pe.Node(interface=niu.IdentityInterface(fields=['fwhm']), name=\"smoothlevels\")\n",
    "\n",
    "smoothlevels.iterables = ('fwhm',[0,4,8,16,32])\n",
    "\n",
    "\n",
    "\n",
    "datasource_cope = pe.Node(interface=nio.DataGrabber(infields=['copenum','regtype'],\n",
    "                    outfields=['copes']),\n",
    "                    name = 'datasource_cope')\n",
    "\n",
    "datasource_cope.inputs.base_directory = config.data['ds005']['datadir']\n",
    "#fixedfx/cope/_copenum_1/_regtype_affine/_subject_id_sub-03/_flameo0/cope1.nii.gz\n",
    "datasource_cope.inputs.template = 'derivatives/fixedfx/cope/_copenum_%d/_regtype_%s/_subject_id_*/_flameo0/cope1.nii.gz'\n",
    "datasource_cope.inputs.template_args = dict(copes=[['copenum','regtype']])\n",
    "datasource_cope.inputs.sort_filelist = True\n",
    "\n",
    "datasource_varcope = pe.Node(interface=nio.DataGrabber(infields=['copenum','regtype'],\n",
    "                    outfields=['varcopes']),\n",
    "                    name = 'datasource_varcope')\n",
    "\n",
    "datasource_varcope.inputs.base_directory = config.data['ds005']['datadir']\n",
    "datasource_varcope.inputs.template = 'derivatives/fixedfx/varcope/_copenum_%d/_regtype_%s/_subject_id_*/_flameo0/varcope1.nii.gz'\n",
    "datasource_varcope.inputs.template_args = dict(varcopes=[['copenum','regtype']])\n",
    "datasource_varcope.inputs.sort_filelist = True\n",
    "\n",
    "\n",
    "copemerge    = pe.MapNode(interface=fsl.Merge(dimension='t'),\n",
    "                          iterfield=['in_files'],\n",
    "                          name=\"copemerge\")\n",
    "\n",
    "varcopemerge = pe.MapNode(interface=fsl.Merge(dimension='t'),\n",
    "                       iterfield=['in_files'],\n",
    "                       name=\"varcopemerge\")\n",
    "\n",
    "mixed_fx.connect(copeinfo,'copenum',datasource_cope,'copenum')\n",
    "mixed_fx.connect(copeinfo,'copenum',datasource_varcope,'copenum')\n",
    "mixed_fx.connect(regtypes,'regtype',datasource_cope,'regtype')\n",
    "mixed_fx.connect(regtypes,'regtype',datasource_varcope,'regtype')\n",
    "\n",
    "\n",
    "\n",
    "mixed_fx.connect(datasource_cope,'copes',copemerge,'in_files')\n",
    "mixed_fx.connect(datasource_varcope,'varcopes',varcopemerge,'in_files')\n",
    "\n",
    "level3model = pe.Node(interface=fsl.L2Model(),\n",
    "                      name='l3model')\n",
    "level3model.inputs.num_copes=16\n",
    "\n",
    "smooth_cope=pe.Node(interface=fsl.utils.Smooth(), name=\"smooth_cope\",iterfield=['copenum','fwhm','regtype'])\n",
    "smooth_varcope=pe.Node(interface=fsl.utils.Smooth(), name=\"smooth_varcope\",iterfield=['copenum','fwhm','regtype'])\n",
    "\n",
    "\n",
    "mixed_fx.connect(copemerge,('merged_file',pickfirst),smooth_cope,'in_file')\n",
    "mixed_fx.connect(varcopemerge,('merged_file',pickfirst),smooth_varcope,'in_file')\n",
    "mixed_fx.connect(smoothlevels,'fwhm',smooth_cope,'fwhm')\n",
    "mixed_fx.connect(smoothlevels,'fwhm',smooth_varcope,'fwhm')\n",
    "\n",
    "\n",
    "flame1 = pe.MapNode(interface=fsl.FLAMEO(run_mode='flame1'), name=\"flame1\",\n",
    "                    iterfield=['cope_file','var_cope_file'])\n",
    "flame1.inputs.mask_file=os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain_mask.nii.gz')\n",
    "#fixed_fx.connect(datasource_mask,'mask',flameo,'mask_file')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mixed_fx.connect(smooth_cope,'smoothed_file',flame1,'cope_file')\n",
    "mixed_fx.connect(smooth_varcope,'smoothed_file',flame1,'var_cope_file')\n",
    "mixed_fx.connect(smooth_cope,'smoothed_file',datasink,'smoothed_cope')\n",
    "\n",
    "\n",
    "mixed_fx.connect(level3model,'design_mat',flame1,'design_file')\n",
    "mixed_fx.connect(level3model,'design_con',flame1,'t_con_file')\n",
    "mixed_fx.connect(level3model,'design_grp',flame1,'cov_split_file')\n",
    "\n",
    "\n",
    "#mixed_fx.connect(flame1,'stats_dir',datasink,'stats')\n",
    "\n",
    "# compute smoothness of residuals\n",
    "\n",
    "smoothest=pe.Node(fsl.SmoothEstimate(),name='smoothest')\n",
    "smoothest.inputs.dof=15\n",
    "smoothest.inputs.mask_file=os.path.join(os.getenv('FSLDIR'),'data/standard/MNI152_T1_2mm_brain_mask.nii.gz')\n",
    "\n",
    "def pickfirst(files):\n",
    "    if isinstance(files, list):\n",
    "        return files[0]\n",
    "    else:\n",
    "        return files\n",
    "\n",
    "\n",
    "mixed_fx.connect(flame1,('res4d',pickfirst),smoothest,'residual_fit_file')\n",
    "\n",
    "# clustering with gaussian random field theory\n",
    "\n",
    "cluster=pe.Node(fsl.Cluster(),name='cluster')\n",
    "cluster.inputs.threshold = 3.0\n",
    "cluster.inputs.pthreshold=0.05\n",
    "cluster.inputs.out_threshold_file='thresh_zstat1.nii.gz'\n",
    "cluster.inputs.out_localmax_txt_file='localmax.txt'\n",
    "\n",
    "mixed_fx.connect(flame1,('zstats',pickfirst),cluster,'in_file')\n",
    "mixed_fx.connect(smoothest,'dlh',cluster,'dlh')\n",
    "mixed_fx.connect(smoothest,'volume',cluster,'volume')\n",
    "\n",
    "mixed_fx.connect(cluster,'threshold_file',datasink,'cluster.thresh_zstat')\n",
    "mixed_fx.connect(cluster,'localmax_txt_file',datasink,'cluster.localmax')\n",
    "\n",
    "\n",
    "\n",
    "mixed_fx.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonparametric correction using randomise\n",
    "\n",
    "Run this outside of nipype because interface is currenly not working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomise_output_dir=os.path.join(config.data['ds005']['datadir'],'derivatives/randomise')\n",
    "if not os.path.exists(randomise_output_dir):\n",
    "    os.mkdir(randomise_output_dir)\n",
    "\n",
    "\n",
    "for cope in range(1,5):\n",
    "    for fwhm in [0,4,8,16,32]:\n",
    "        for regtype in ['affine','ants']:\n",
    "            copefile=os.path.join(config.data['ds005']['datadir'],\n",
    "                    'derivatives/smoothed_cope/_copenum_%d/_regtype_%s/_fwhm_%d/cope1_merged_smooth.nii.gz'%(cope,regtype,fwhm))\n",
    "            print(copefile)\n",
    "            assert os.path.exists(copefile)\n",
    "            outdir=os.path.join(randomise_output_dir,'_copenum_%d/_regtype_%s/_fwhm_%d'%(cope,regtype,fwhm))\n",
    "            if not os.path.exists(outdir):\n",
    "                os.makedirs(outdir)\n",
    "            nperms=2500\n",
    "            cmd='randomise -i %s -o \"%s/randomise_\" -c 3.00 -C 3.00 -n %d -1 -T -v 10'%(copefile,outdir,nperms)\n",
    "            print(cmd)\n",
    "            if not os.path.exists(os.path.join(outdir,'randomise__tstat1.nii.gz')):\n",
    "                !{cmd}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Statistical maps using linear registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
